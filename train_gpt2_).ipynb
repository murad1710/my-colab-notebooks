{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62hJinCa6mHA"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Point:\n",
        "    def __init__(self, x: int, y: int):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Point(x={self.x}, y={self.y})\"\n",
        "\n",
        "point = Point(3, 5)\n",
        "print(point)  # Output: Point(x=3, y=5)\n",
        "\n",
        "\n",
        "\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass()\n",
        "class Point:\n",
        "    x: int\n",
        "    y: int\n",
        "\n",
        "point = Point(3, 5)\n",
        "print(point)  # Output: Point(x=3, y=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyOM-aOJ8pNh",
        "outputId": "eaea3620-539f-4f62-eeb0-941d1d53fdb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Point(x=3, y=5)\n",
            "Point(x=3, y=5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        #output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT=1\n",
        "        #Regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        #bad naming\n",
        "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).\n",
        "                             view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        #calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        #compute attention scores\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.c_proj(y)\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "QTsRdT2xF0Ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate= 'tanh')\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT=1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5keC897nC4ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CasualSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "FrADitg5CC9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768"
      ],
      "metadata": {
        "id": "LxiRmSuy6voh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList(Block(config) for _ in range(config.n_layer)),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias = False)\n",
        "\n",
        "        self.transformer.wte.weight=self.lm_head.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "\n",
        "    def _init_weights(self,module):\n",
        "      if isinstance(module,nn.Linear):\n",
        "        std=0.02\n",
        "        if hasattr(module,'NANOGPT_SCALE_INIT'):\n",
        "          std= (2* self.config.n_layer)**-0.5\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "        if module.bias is not None:\n",
        "          torch.nn.init.zeros_(module.bias)\n",
        "      elif isinstance(module,nn.Embedding):\n",
        "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "        tok_emb = self.transformer.wte(idx)\n",
        "        pos_emb = self.transformer.wpe(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        loss=None\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(self.lm_head(x.view(-1, x.size(-1))), targets.view(-1))\n",
        "        logits = self.lm_head(x)\n",
        "        return logits,loss\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        '''Loads pretrained GPT-2 model weights from huggingface'''\n",
        "        assert model_type in ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n",
        "        from transformers import GPT2LMHeadModel\n",
        "        print('Loading weights from huggingface...')\n",
        "\n",
        "        config_args = {\n",
        "            \"gpt2\":  dict(n_layer=12, n_head=12, n_embd=768), #124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024), #355M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280), #774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600), #1558M params\n",
        "        }[model_type]\n",
        "        config_args['vocab_size'] = 50257\n",
        "        config_args['block_size'] = 1024\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')]\n",
        "\n",
        "        #init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        #copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')]\n",
        "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')]\n",
        "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        assert len(sd_keys) == len(sd_keys_hf), f'mismatched number of keys: {len(sd_keys)} vs {len(sd_keys_hf)}'\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape, f'mismatched shape for {k}: {sd[k].shape} vs {sd_hf[k].shape}'\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                assert sd_hf[k].shape == sd[k].shape, f'mismatched shape for {k}: {sd[k].shape} vs {sd_hf[k].shape}'\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "        return model\n",
        "\n",
        "model = GPT.from_pretrained('gpt2')\n",
        "print(model)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6Ra0PfSI69le",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94eaf6a5-8bc1-4f6f-b88e-902e68f21946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights from huggingface...\n",
            "GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CasualSelfAttention(\n",
            "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): MLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (gelu): GELU(approximate='tanh')\n",
            "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = 'mps'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41MoFBEv1ZYx",
        "outputId": "537962a9-548b-43b0-bc85-39e912364af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqgoRea8vU9L",
        "outputId": "3b192add-748a-4c5b-89ef-2828aabaffb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50304, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CasualSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69BzCJYdv-Ey",
        "outputId": "1425bd11-f78a-489a-a17d-9b11e2e36869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tiktoken\n",
        "# enc = tiktoken.get_encoding(\"gpt2\")\n",
        "# tokens = enc.encode(\"Hello, I'm a language model\")\n",
        "# tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "# tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "# x = tokens.to(device)\n",
        "\n",
        "# torch.manual_seed(42)\n",
        "# torch.cuda.manual_seed(42)\n",
        "# while x.size(1) < max_length:\n",
        "#     with torch.no_grad():\n",
        "#         logits = model(x)\n",
        "#         # logits = logits[:, -1, :]\n",
        "#         probs = F.softmax(logits, dim=-1)\n",
        "#         # next_token = torch.multinomial(probs, num_samples=1)\n",
        "#         # tokens = torch.cat([tokens, next_token], dim=1)\n",
        "#         topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1)\n",
        "#         ix = torch.multinomial(topk_probs, 1)\n",
        "#         xcol = torch.gather(topk_indices, -1, ix)\n",
        "#         x = torch.cat([x, xcol], dim=1)"
      ],
      "metadata": {
        "id": "NzX6Fx8WNZX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print('>', decoded)"
      ],
      "metadata": {
        "id": "RbGG-Q9fv9BW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "9f67ec4e-c95a-455f-9311-c63acaa06860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> IOLANUS:\n",
            "To brag unto them, thus I did, and thus;\n",
            "Show them the unaching scars which I should hide\n",
            "> In wholesome manner.\n",
            "\n",
            "CORIOLANUS:\n",
            "Bid them wash their faces\n",
            "And keep their teeth clean.\n",
            "So,\n",
            ">  the tribunes\n",
            "Endue you with the people's voice: remains\n",
            "That, in the official marks invested, you\n",
            "Anon do meet the\n",
            ">  deny him:\n",
            "I'll have five hundred voices of that sound.\n",
            "\n",
            "First Citizen:\n",
            "I twice five hundred and their friends to piece '\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 4 is out of bounds for dimension 0 with size 4",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-691324176498>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for dimension 0 with size 4"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/build-nanogpt/refs/heads/master/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JulryCfi3lWX",
        "outputId": "2600eb22-4f0f-4d00-c024-590ae7aa13b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-04 13:04:54--  https://raw.githubusercontent.com/karpathy/build-nanogpt/refs/heads/master/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.3’\n",
            "\n",
            "\rinput.txt.3           0%[                    ]       0  --.-KB/s               \rinput.txt.3         100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-02-04 13:04:54 (31.7 MB/s) - ‘input.txt.3’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "with open('input.txt', 'r') as f:\n",
        "  text= f.read()\n",
        "text= text[:1000]\n",
        "tokens = enc.encode(text)\n",
        "\n",
        "B,T=4,32\n",
        "buf=torch.tensor(tokens[:B*T+1])\n",
        "buf=buf.to(device)\n",
        "x= buf[:-1].view(B,T)\n",
        "y= buf[1:].view(B,T)\n",
        "model=GPT(GPTConfig)\n",
        "model.to(device)\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
        "\n",
        "for i in range (50):\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  logits,loss=model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step{i},loss:{loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mF-ot9pl35eU",
        "outputId": "1b458fcf-078e-45f1-8ba1-9568cd908bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step0,loss:11.000419616699219\n",
            "step1,loss:8.208377838134766\n",
            "step2,loss:7.029065132141113\n",
            "step3,loss:6.53325891494751\n",
            "step4,loss:5.4355387687683105\n",
            "step5,loss:4.640581130981445\n",
            "step6,loss:4.1531901359558105\n",
            "step7,loss:3.8315253257751465\n",
            "step8,loss:3.594635009765625\n",
            "step9,loss:3.465388536453247\n",
            "step10,loss:3.3501698970794678\n",
            "step11,loss:3.192009449005127\n",
            "step12,loss:3.152534008026123\n",
            "step13,loss:3.2802484035491943\n",
            "step14,loss:3.3397154808044434\n",
            "step15,loss:3.2563881874084473\n",
            "step16,loss:3.168583393096924\n",
            "step17,loss:3.121976852416992\n",
            "step18,loss:3.064889669418335\n",
            "step19,loss:2.958860397338867\n",
            "step20,loss:2.861670732498169\n",
            "step21,loss:2.792046546936035\n",
            "step22,loss:2.7135705947875977\n",
            "step23,loss:2.628185987472534\n",
            "step24,loss:2.5604312419891357\n",
            "step25,loss:2.458855152130127\n",
            "step26,loss:2.3569202423095703\n",
            "step27,loss:2.24489426612854\n",
            "step28,loss:2.154266595840454\n",
            "step29,loss:2.0578033924102783\n",
            "step30,loss:1.9585514068603516\n",
            "step31,loss:1.8522930145263672\n",
            "step32,loss:1.740715742111206\n",
            "step33,loss:1.6451233625411987\n",
            "step34,loss:1.5460708141326904\n",
            "step35,loss:1.4458574056625366\n",
            "step36,loss:1.3693134784698486\n",
            "step37,loss:1.3240455389022827\n",
            "step38,loss:1.2071071863174438\n",
            "step39,loss:1.0764265060424805\n",
            "step40,loss:1.0105643272399902\n",
            "step41,loss:0.9121036529541016\n",
            "step42,loss:0.8045259118080139\n",
            "step43,loss:0.7150309681892395\n",
            "step44,loss:0.6436391472816467\n",
            "step45,loss:0.5520056486129761\n",
            "step46,loss:0.5194314122200012\n",
            "step47,loss:0.4739416539669037\n",
            "step48,loss:0.3867977261543274\n",
            "step49,loss:0.3273155689239502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B,T):\n",
        "        self.B=B\n",
        "        self.T=T\n",
        "\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text= f.read()\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens=torch.tensor(tokens)\n",
        "        print(f\"loaded{len(self.tokens)} tokens\")\n",
        "        print(f\"1 epoch ={len(self.tokens)// (B*T)} batches\")\n",
        "\n",
        "        self.current_position=0\n",
        "\n",
        "    def next_batch(self):\n",
        "       B,T = self.B, self.T\n",
        "       buf= self.tokens[self.current_position:self.current_position+B*T+1]\n",
        "       x= buf[:-1].view(B,T)\n",
        "       y= buf[1:].view(B,T)\n",
        "       self.current_position+=B*T+1\n",
        "\n",
        "       if self.current_position +(B*T+1) >= len(self.tokens):\n",
        "         self.current_position=0\n",
        "       return x,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WsSvi43-9Y5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_load=DataLoaderLite(4,32)\n",
        "\n",
        "optimizer= torch.optim.AdamW(model.parameters(),lr=1e-3)\n",
        "for i in range(50):\n",
        "  x,y=train_load.next_batch()\n",
        "  x= x.to(device)\n",
        "  y=y.to(device)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  logits,loss=model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"step{i},loss:{loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR63tw9jMoth",
        "outputId": "662843af-7e3c-44e1-f573-3986d8be92d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded338025 tokens\n",
            "1 epoch =2640 batches\n",
            "step0,loss:0.28227752447128296\n",
            "step1,loss:12.147817611694336\n",
            "step2,loss:10.2726469039917\n",
            "step3,loss:11.064226150512695\n",
            "step4,loss:9.064347267150879\n",
            "step5,loss:8.45627212524414\n",
            "step6,loss:8.847795486450195\n",
            "step7,loss:8.248506546020508\n",
            "step8,loss:8.024679183959961\n",
            "step9,loss:7.668798923492432\n",
            "step10,loss:7.934527397155762\n",
            "step11,loss:6.717639923095703\n",
            "step12,loss:7.085093021392822\n",
            "step13,loss:7.089598178863525\n",
            "step14,loss:7.1593194007873535\n",
            "step15,loss:7.196556568145752\n",
            "step16,loss:7.771577835083008\n",
            "step17,loss:8.556546211242676\n",
            "step18,loss:6.815072059631348\n",
            "step19,loss:8.091900825500488\n",
            "step20,loss:7.528286457061768\n",
            "step21,loss:7.444916725158691\n",
            "step22,loss:6.6459197998046875\n",
            "step23,loss:6.894480228424072\n",
            "step24,loss:6.5769124031066895\n",
            "step25,loss:6.686009407043457\n",
            "step26,loss:6.937647342681885\n",
            "step27,loss:7.813632488250732\n",
            "step28,loss:6.798986911773682\n",
            "step29,loss:6.8111653327941895\n",
            "step30,loss:6.875070571899414\n",
            "step31,loss:7.156610488891602\n",
            "step32,loss:6.463193416595459\n",
            "step33,loss:7.87858247756958\n",
            "step34,loss:8.095812797546387\n",
            "step35,loss:8.098018646240234\n",
            "step36,loss:7.408615589141846\n",
            "step37,loss:7.948853969573975\n",
            "step38,loss:7.161536693572998\n",
            "step39,loss:7.196244239807129\n",
            "step40,loss:6.8522629737854\n",
            "step41,loss:6.5930399894714355\n",
            "step42,loss:6.6643171310424805\n",
            "step43,loss:5.823880672454834\n",
            "step44,loss:6.48749303817749\n",
            "step45,loss:6.229504585266113\n",
            "step46,loss:6.28143835067749\n",
            "step47,loss:7.03816556930542\n",
            "step48,loss:6.995795726776123\n",
            "step49,loss:7.146584510803223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "device= 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device= 'cuda'\n",
        "elif torch.backends.mps.is_available():\n",
        "    device= 'mps'\n",
        "print(device)\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "train_loader=DataLoaderLite(B=4,T=1024)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "model=GPT(GPTConfig())\n",
        "model.to(device)\n",
        "model=torch.compile(model)\n",
        "\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "optimizer=torch.optim.AdamW(model.parameters(),lr=3e-4, betas = (0.9, 0.95), eps = 1e-8)\n",
        "for step in range(5):\n",
        "  t0=time.time()\n",
        "  x,y=train_loader.next_batch()\n",
        "  x=x.to(device)\n",
        "  y=y.to(device)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  with torch.autocast(device_type=device):\n",
        "     logits,loss=model(x,y)\n",
        "\n",
        "  loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  lr = get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "      param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  torch.cuda.synchronize()\n",
        "  t1=time.time()\n",
        "  dt=(t1-t0)*1000\n",
        "  tokens_per_sec=(train_loader.B * train_loader.T)/ (t1-t0)\n",
        "  print(f\"step{step}, loss:{loss.item()}, |lr {lr:.2f} | norm {norm:.4f} | time:{dt:.2f}ms, tok/sec:{tokens_per_sec:4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTQAx9mvRPW0",
        "outputId": "f4c0277d-556b-4861-b936-c2ec51de7115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "loaded338025 tokens\n",
            "1 epoch =82 batches\n",
            "step0, loss:10.96420669555664, |lr 0.00 | norm 10.4052 | time:324.36ms, tok/sec:12627.818446\n",
            "step1, loss:10.009471893310547, |lr 0.00 | norm 6.0850 | time:269.63ms, tok/sec:15191.391936\n",
            "step2, loss:9.551011085510254, |lr 0.00 | norm 3.4422 | time:267.51ms, tok/sec:15311.771780\n",
            "step3, loss:9.17956256866455, |lr 0.00 | norm 3.6222 | time:264.64ms, tok/sec:15477.777944\n",
            "step4, loss:8.897397994995117, |lr 0.00 | norm 2.6608 | time:275.77ms, tok/sec:14853.152544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t5zUA-x6Kk4O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}